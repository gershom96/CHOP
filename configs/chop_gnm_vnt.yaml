# NOTE: this model uses private datasets

project_name: chop-gnm
run_name: gnm

# training setup
use_wandb: True # set to false if you don't want to log to wandb
train: True
batch_size: 8192
eval_batch_size: 8192
epochs: 30
gpu_ids: [0, 1, 2, 3] 
num_workers: 8
lr: 7e-4
optimizer: adam
seed: 0

# model params
model_type: gnm
obs_encoding_size: 1024
goal_encoding_size: 1024

# normalization for the action space
normalize: True

# context
context_type: temporal # [temporal, randomized]
context_size: 5

# tradeoff between action and distance prediction loss
alpha: 0.5

# distance bounds for distance and action and distance predictions 
distance:
  min_dist_cat: 0
  max_dist_cat: 20
action:
  min_dist_cat: 2
  max_dist_cat: 10
close_far_threshold: 10 # distance threshold used to seperate the close and the far  subgoals that are sampled per datapoint

# action output params
len_traj_pred: 5
learn_angle: True

# dataset specific parameters
image_size: [85, 64] # width, height
goal_type: "image"

load_run: True
load_pretrained: True
pretrained_model_path: ./weights/gnm.pth
load_opt_sched: False

datasets:
  scand_a:
    data_root: ./data/lora-data
    train: ./data/lora-data/train.json
    test: ./data/lora-data/test.json
    image_root: /fs/gamma-datasets/SCAND/images
    goals_per_obs: 1 # how many goals are sampled per observation
    negative_mining: True # negative mining from the ViNG paper (Shah et al.)  
    data_split_folder: ./data/lora-data/
    negative_mining_pct: 0.05
    metric_waypoint_spacing: 0.38

# logging stuff
## =0 turns off
print_log_freq: 100 # in iterations
image_log_freq: 1000 # in iterations
num_images_log: 8 # number of images to log in a logging iteration
pairwise_test_freq: 20 # in epochs
wandb_log_freq: 10 # in iterations
eval_freq: 1 # in epochs
